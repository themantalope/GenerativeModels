{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa57bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6286986e",
   "metadata": {},
   "source": [
    "# Denoising Diffusion Probabilistic Model on 3D data\n",
    "\n",
    "This tutorial illustrates how to use MONAI for training a denoising diffusion probabilistic model (DDPM)[1] to create synthetic 3D images.\n",
    "\n",
    "[1] - [Ho et al. \"Denoising Diffusion Probabilistic Models\"](https://arxiv.org/abs/2006.11239)\n",
    "\n",
    "\n",
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f96b6f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, tqdm]\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc01d24",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdea37d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-28 05:17:25,143 - A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: No module named 'triton'\n",
      "MONAI version: 1.2.dev2304\n",
      "Numpy version: 1.22.2\n",
      "Pytorch version: 1.13.1+cu117\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 9a57be5aab9f2c2a134768c0c146399150e247a0\n",
      "MONAI __file__: /opt/conda/lib/python3.8/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.4.10\n",
      "ITK version: 5.3.0\n",
      "Nibabel version: 5.1.0\n",
      "scikit-image version: 0.20.0\n",
      "Pillow version: 9.0.1\n",
      "Tensorboard version: 2.10.1\n",
      "gdown version: 4.7.1\n",
      "TorchVision version: 0.14.1+cu117\n",
      "tqdm version: 4.64.1\n",
      "lmdb version: 1.3.0\n",
      "psutil version: 5.9.2\n",
      "pandas version: 1.4.4\n",
      "einops version: 0.6.1\n",
      "transformers version: 4.21.3\n",
      "mlflow version: 2.3.0\n",
      "pynrrd version: 1.0.0\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from monai.apps import DecathlonDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader\n",
    "from monai.transforms import AddChanneld, CenterSpatialCropd, Compose, Lambdad, LoadImaged, Resized, ScaleIntensityd\n",
    "from monai.utils import set_determinism\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "\n",
    "from generative.inferers import DiffusionInferer\n",
    "from generative.networks.nets import DiffusionModelUNet\n",
    "from generative.networks.schedulers import DDPMScheduler, DDIMScheduler\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e37a43",
   "metadata": {},
   "source": [
    "## Setup data directory\n",
    "\n",
    "You can specify a directory with the MONAI_DATA_DIRECTORY environment variable.\n",
    "\n",
    "This allows you to save results and reuse downloads.\n",
    "\n",
    "If not specified a temporary directory will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c38b4c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmpg9bnwx7n\n"
     ]
    }
   ],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41af1391",
   "metadata": {},
   "source": [
    "## Set deterministic training for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "515d8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d8c601",
   "metadata": {},
   "source": [
    "## Setup Decathlon Dataset and training and validation data loaders\n",
    "\n",
    "In this tutorial, we will use the 3D T1 weighted brain images from the [2016 and 2017 Brain Tumor Segmentation (BraTS) challenges](https://www.med.upenn.edu/sbia/brats2017/data.html). This dataset can be easily downloaded using the [DecathlonDataset](https://docs.monai.io/en/stable/apps.html#monai.apps.DecathlonDataset) from MONAI (`task=\"Task01_BrainTumour\"`). To load the training and validation images, we are using the `data_transform` transformations that are responsible for the following:\n",
    "\n",
    "1. `LoadImaged`:  Loads the brain images from files.\n",
    "2. `Lambdad`: Choose channel 1 of the image, which is the T1-weighted image.\n",
    "3. `AddChanneld`: Add the channel dimension of the input data.\n",
    "4. `ScaleIntensityd`: Apply a min-max scaling in the intensity values of each image to be in the `[0, 1]` range.\n",
    "5. `CenterSpatialCropd`: Crop the background of the images using a roi of size `[160, 200, 155]`.\n",
    "6. `Resized`: Resize the images to a volume with size `[32, 40, 32]`.\n",
    "\n",
    "For the data loader, we are using mini-batches of 8 images, which consumes about 21GB of GPU memory during training. Please, reduce this value to run on smaller GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f640d7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<class 'monai.transforms.utility.array.AddChannel'>: Class `AddChannel` has been deprecated since version 0.8. please use MetaTensor data type and monai.transforms.EnsureChannelFirst instead.\n"
     ]
    }
   ],
   "source": [
    "data_transform = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\"]),\n",
    "        Lambdad(keys=\"image\", func=lambda x: x[:, :, :, 1]),\n",
    "        AddChanneld(keys=[\"image\"]),\n",
    "        ScaleIntensityd(keys=[\"image\"]),\n",
    "        CenterSpatialCropd(keys=[\"image\"], roi_size=[160, 200, 155]),\n",
    "        Resized(keys=[\"image\"], spatial_size=(32, 40, 32)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f7b8d3-7ecf-40c8-ac20-f3819595b6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a loader\n",
    "\n",
    "def resample_to_size(in_subject, desired_size):\n",
    "    # need to first figure out the right spacing\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddd61e60",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task01_BrainTumour.tar: 7.09GB [06:57, 18.2MB/s]                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-28 05:24:34,858 - INFO - Downloaded: /tmp/tmpg9bnwx7n/Task01_BrainTumour.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-28 05:24:47,318 - INFO - Verified 'Task01_BrainTumour.tar', md5: 240a19d752f0d9e9101544901065d872.\n",
      "2023-04-28 05:24:47,322 - INFO - Writing into directory: /tmp/tmpg9bnwx7n.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset:  28%|██▊       | 109/388 [01:38<04:13,  1.10it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.8/multiprocessing/pool.py:851\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 851\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_items\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopleft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m \u001b[43mDecathlonDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTask01_BrainTumour\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_ds, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, persistent_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m val_ds \u001b[38;5;241m=\u001b[39m DecathlonDataset(\n\u001b[1;32m      8\u001b[0m     root_dir\u001b[38;5;241m=\u001b[39mroot_dir, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask01_BrainTumour\u001b[39m\u001b[38;5;124m\"\u001b[39m, transform\u001b[38;5;241m=\u001b[39mdata_transform, section\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      9\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/monai/apps/datasets.py:344\u001b[0m, in \u001b[0;36mDecathlonDataset.__init__\u001b[0;34m(self, root_dir, task, section, transform, download, seed, val_frac, cache_num, cache_rate, num_workers, progress, copy_cache, as_contiguous, runtime_cache)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transform \u001b[38;5;241m==\u001b[39m ():\n\u001b[1;32m    343\u001b[0m     transform \u001b[38;5;241m=\u001b[39m LoadImaged([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 344\u001b[0m \u001b[43mCacheDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_contiguous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_contiguous\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mruntime_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mruntime_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/monai/data/dataset.py:817\u001b[0m, in \u001b[0;36mCacheDataset.__init__\u001b[0;34m(self, data, transform, cache_num, cache_rate, num_workers, progress, copy_cache, as_contiguous, hash_as_key, hash_func, runtime_cache)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m|\u001b[39m ListProxy \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hash_keys: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 817\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/monai/data/dataset.py:844\u001b[0m, in \u001b[0;36mCacheDataset.set_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    841\u001b[0m     indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_num))\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mruntime_cache \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# prepare cache content immediately\u001b[39;00m\n\u001b[0;32m--> 844\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fill_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mruntime_cache, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mruntime_cache:\n\u001b[1;32m    847\u001b[0m     \u001b[38;5;66;03m# this must be in the main process, not in dataloader's workers\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/monai/data/dataset.py:873\u001b[0m, in \u001b[0;36mCacheDataset._fill_cache\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_workers) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogress \u001b[38;5;129;01mand\u001b[39;00m has_tqdm:\n\u001b[0;32m--> 873\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_cache_item\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLoading dataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(p\u001b[38;5;241m.\u001b[39mimap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_cache_item, indices))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/multiprocessing/pool.py:856\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 856\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    858\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mpopleft()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_ds = DecathlonDataset(\n",
    "    root_dir=root_dir, task=\"Task01_BrainTumour\", transform=data_transform, section=\"training\", download=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=8, persistent_workers=True)\n",
    "\n",
    "val_ds = DecathlonDataset(\n",
    "    root_dir=root_dir, task=\"Task01_BrainTumour\", transform=data_transform, section=\"validation\", download=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(val_ds, batch_size=8, shuffle=False, num_workers=8, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50efe5ef",
   "metadata": {},
   "source": [
    "### Visualization of the training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffb4abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(1, 4, figsize=(10, 6))\n",
    "for i in range(4):\n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    plt.imshow(train_ds[i * 20][\"image\"][0, :, :, 15].detach().cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22296e5",
   "metadata": {},
   "source": [
    "### Define network, scheduler, optimizer, and inferer\n",
    "\n",
    "We will use a DDPM in this example; for that, we need to define a `DiffusionModelUNet` network that will have as input the noisy images and the values for the timestep `t`, and it will predict the noise that is present in the image.\n",
    "\n",
    "In this example, we have a network with three levels (with 256, 256, and 512 channels in each). In every level, we will have two residual blocks, and only the last one will have an attention block with a single attention head (with 512 channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d499f7b1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = DiffusionModelUNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    num_channels=[256, 256, 512],\n",
    "    attention_levels=[False, False, True],\n",
    "    num_head_channels=[0, 0, 512],\n",
    "    num_res_blocks=2,\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ad91ff",
   "metadata": {},
   "source": [
    "Together with our U-net, we need to define the Noise Scheduler for the diffusion model. This scheduler is responsible for defining the amount of noise that should be added in each timestep `t` of the diffusion model's Markov chain. Besides that, it has the operations to perform the reverse process, which will remove the noise of the images (a.k.a. denoising process). In this case, we are using a `DDPMScheduler`. Here we are using 1000 timesteps and a `scaled_linear` profile for the beta values (proposed in [Rombach et al. \"High-Resolution Image Synthesis with Latent Diffusion Models\"](https://arxiv.org/abs/2112.10752)). This profile had better results than the `linear, proposed in the original DDPM's paper. In `beta_start` and `beta_end`, we define the limits for the beta values. These are important to determine how accentuated is the addition of noise in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1de5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule=\"scaled_linear\", beta_start=0.0005, beta_end=0.0195)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d3e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scheduler.alphas_cumprod.cpu(), color=(2 / 255, 163 / 255, 163 / 255), linewidth=2)\n",
    "plt.xlabel(\"Timestep [t]\")\n",
    "plt.ylabel(\"alpha cumprod\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9125f7c8",
   "metadata": {},
   "source": [
    "Finally, we define the Inferer, which contains functions that will help during the training and sampling of the model, and the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8685da6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferer = DiffusionInferer(scheduler)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f371ad8",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "In this part, we will train the diffusion model to predict the noise added to the images. For this, we are using an MSE loss between the prediction and the original noise. During the training, we are also sampling brain images to evaluate the evolution of the model. In this training, we use Automatic Mixed Precision to save memory and speed up the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd10b595",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "n_epochs = 150\n",
    "val_interval = 25\n",
    "epoch_loss_list = []\n",
    "val_epoch_loss_list = []\n",
    "\n",
    "scaler = GradScaler()\n",
    "total_start = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=70)\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    for step, batch in progress_bar:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast(enabled=True):\n",
    "            # Generate random noise\n",
    "            noise = torch.randn_like(images).to(device)\n",
    "\n",
    "            # Create timesteps\n",
    "            timesteps = torch.randint(\n",
    "                0, inferer.scheduler.num_train_timesteps, (images.shape[0],), device=images.device\n",
    "            ).long()\n",
    "\n",
    "            # Get model prediction\n",
    "            noise_pred = inferer(inputs=images, diffusion_model=model, noise=noise, timesteps=timesteps)\n",
    "\n",
    "            loss = F.mse_loss(noise_pred.float(), noise.float())\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        progress_bar.set_postfix({\"loss\": epoch_loss / (step + 1)})\n",
    "    epoch_loss_list.append(epoch_loss / (step + 1))\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        val_epoch_loss = 0\n",
    "        for step, batch in enumerate(val_loader):\n",
    "            images = batch[\"image\"].to(device)\n",
    "            noise = torch.randn_like(images).to(device)\n",
    "            with torch.no_grad():\n",
    "                with autocast(enabled=True):\n",
    "                    timesteps = torch.randint(\n",
    "                        0, inferer.scheduler.num_train_timesteps, (images.shape[0],), device=images.device\n",
    "                    ).long()\n",
    "\n",
    "                    # Get model prediction\n",
    "                    noise_pred = inferer(inputs=images, diffusion_model=model, noise=noise, timesteps=timesteps)\n",
    "                    val_loss = F.mse_loss(noise_pred.float(), noise.float())\n",
    "\n",
    "            val_epoch_loss += val_loss.item()\n",
    "            progress_bar.set_postfix({\"val_loss\": val_epoch_loss / (step + 1)})\n",
    "        val_epoch_loss_list.append(val_epoch_loss / (step + 1))\n",
    "\n",
    "        # Sampling image during training\n",
    "        image = torch.randn((1, 1, 32, 40, 32))\n",
    "        image = image.to(device)\n",
    "        scheduler.set_timesteps(num_inference_steps=1000)\n",
    "        with autocast(enabled=True):\n",
    "            image = inferer.sample(input_noise=image, diffusion_model=model, scheduler=scheduler)\n",
    "\n",
    "        plt.figure(figsize=(2, 2))\n",
    "        plt.imshow(image[0, 0, :, :, 15].cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "        plt.tight_layout()\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"train completed, total time: {total_time}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e263b67",
   "metadata": {},
   "source": [
    "### Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7520419",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-v0_8\")\n",
    "plt.title(\"Learning Curves\", fontsize=20)\n",
    "plt.plot(np.linspace(1, n_epochs, n_epochs), epoch_loss_list, color=\"C0\", linewidth=2.0, label=\"Train\")\n",
    "plt.plot(\n",
    "    np.linspace(val_interval, n_epochs, int(n_epochs / val_interval)),\n",
    "    val_epoch_loss_list,\n",
    "    color=\"C1\",\n",
    "    linewidth=2.0,\n",
    "    label=\"Validation\",\n",
    ")\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.xlabel(\"Epochs\", fontsize=16)\n",
    "plt.ylabel(\"Loss\", fontsize=16)\n",
    "plt.legend(prop={\"size\": 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38724c9b",
   "metadata": {},
   "source": [
    "## Sampling Brain Image\n",
    "\n",
    "In order to sample the brain images, we need to pass the model an image containing just noise and use it to remove the noise of the image iteratively. For that, we will use the `.sample()` function of the `inferer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092eb6a0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "noise = torch.randn((1, 1, 32, 40, 32))\n",
    "noise = noise.to(device)\n",
    "scheduler.set_timesteps(num_inference_steps=1000)\n",
    "image = inferer.sample(input_noise=noise, diffusion_model=model, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc3e69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"default\")\n",
    "plotting_image_0 = np.concatenate([image[0, 0, :, :, 15].cpu(), np.flipud(image[0, 0, :, 20, :].cpu().T)], axis=1)\n",
    "plotting_image_1 = np.concatenate([np.flipud(image[0, 0, 15, :, :].cpu().T), np.zeros((32, 32))], axis=1)\n",
    "plt.imshow(np.concatenate([plotting_image_0, plotting_image_1], axis=0), vmin=0, vmax=1, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0acc27a",
   "metadata": {},
   "source": [
    "### Sampling with Denoising Diffusion Implicit Model Scheduler\n",
    "\n",
    "Recent papers have proposed different ways to improve the sampling speed by using fewer steps in the denoising process. In this example, we are using a `DDIMScheduler` (from [Song et al. \"Denoising Diffusion Implicit Models\"](https://arxiv.org/abs/2010.02502)) to reduce the original number of steps from 1000 to 250."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e43b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_ddim = DDIMScheduler(\n",
    "    num_train_timesteps=1000, beta_schedule=\"scaled_linear\", beta_start=0.0005, beta_end=0.0195, clip_sample=False\n",
    ")\n",
    "\n",
    "scheduler_ddim.set_timesteps(num_inference_steps=250)\n",
    "\n",
    "model.eval()\n",
    "noise = torch.randn((1, 1, 32, 40, 32))\n",
    "noise = noise.to(device)\n",
    "\n",
    "image = inferer.sample(input_noise=noise, diffusion_model=model, scheduler=scheduler_ddim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f93ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"default\")\n",
    "plotting_image_0 = np.concatenate([image[0, 0, :, :, 15].cpu(), np.flipud(image[0, 0, :, 20, :].cpu().T)], axis=1)\n",
    "plotting_image_1 = np.concatenate([np.flipud(image[0, 0, 15, :, :].cpu().T), np.zeros((32, 32))], axis=1)\n",
    "plt.imshow(np.concatenate([plotting_image_0, plotting_image_1], axis=0), vmin=0, vmax=1, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39c881c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "py:percent,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
